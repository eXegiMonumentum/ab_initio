
# ğŸ§  Jak model rozpoznaje rÃ³Å¼ne gesty?

---

## ğŸ” Model wie, ktÃ³ry gest jest ktÃ³ry dziÄ™ki etykietom (`y`)

W `prepare_gesture_data.py` kluczowa sekcja:

```python
    gestures = {
        "shake": 0,
        "wave": 1,
        "stop": 2,
        "wave_both": 3,
        "freeze": 4,
        "wiggle_V": 5,
        "shake_sidechains": 6,
        "rebuild": 7,
        "zoom_in": 8,
        "zoom_out": 9,
    }
```

Dla kaÅ¼dego gestu:
- `shake` dostaje etykietÄ™ `0`
- `wave` dostaje etykietÄ™ `1`

Potem, jak skrypt przetwarza CSV-y z katalogu `gestures_data/shake/`, to do kaÅ¼dego przykÅ‚adu przypisuje label `0`.

**Dane wynikowe:**
- `X` â€“ przykÅ‚ady: np. 60 sekwencji o ksztaÅ‚cie `(100, 63)`
- `y` â€“ etykiety: np. `[0, 0, 0, ..., 1, 1, 1]`

---

## ğŸ§  Trenowanie modelu uczy go rozrÃ³Å¼niaÄ‡ te etykiety

W `train_gesture_lstm.py`, model trenuje siÄ™ tak:

```python
outputs = model(batch_X)  # prognozy dla batcha
loss = criterion(outputs, batch_y)  # porÃ³wnanie z prawdziwÄ… etykietÄ…
```

Model patrzy na sekwencjÄ™ 3D punktÃ³w dÅ‚oni i uczy siÄ™:
> â€Aha, jeÅ›li palce drgajÄ… w ten sposÃ³b â†’ to shake (0), a jeÅ›li falujÄ… â†’ to wave (1)â€.

---

## ğŸ” W czasie predykcji na Å¼ywo (`predict_live.py`)

- Model analizuje sekwencjÄ™ `100` klatek (czyli `100x63` wejÅ›Ä‡).
- Wypluwa wynik np. `[0.01, 0.98]` â†’ czyli prawdopodobieÅ„stwo dla klas `shake`, `wave`.
- Z `torch.argmax()` wybiera najbardziej prawdopodobnÄ… klasÄ™.

---

## âœ… Podsumowanie

â¡ï¸ Wszystkie dane sÄ… Å‚adowane naraz, **ale z przypisanÄ… etykietÄ… (klasÄ…)**, wiÄ™c model wie, co jest czym.  
â¡ï¸ Trening uczy model rÃ³Å¼nic miÄ™dzy ruchami.  
â¡ï¸ Predykcja sprawdza, ktÃ³ry ruch jest najbardziej podobny do zapamiÄ™tanych gestÃ³w.
