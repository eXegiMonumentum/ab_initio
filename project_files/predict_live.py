
"""
1ï¸âƒ£ Nagrywanie danych (gesture_record.py)
2ï¸âƒ£ Przygotowanie danych (prepare_gesture_data.py)
3ï¸âƒ£ Trening modelu (train_gesture_lstm.py)
4ï¸âƒ£ Ewaluacja modelu (evaluate_model.py)
5ï¸âƒ£ ğŸ”œ (opcjonalnie) Rozpoznawanie gestÃ³w na Å¼ywo z kamery (predict_live.py)

ğŸ›¡ï¸ [NowoÅ›Ä‡] System zostaÅ‚ rozszerzony o zabezpieczenia przed niekontrolowanym wywoÅ‚ywaniem skrÃ³tÃ³w:

- âœ‹ Minimalna liczba stabilnych rozpoznaÅ„ (np. 5x z rzÄ™du ten sam gest), zanim nastÄ…pi akcja.
- ğŸ•’ Cooldown czasowy â€“ ogranicza czÄ™stotliwoÅ›Ä‡ akcji (np. 1 akcja co 2 sekundy).
- ğŸ” Pomijanie powtarzajÄ…cych siÄ™ predykcji tego samego gestu.
- âœ… UÅ¼ycie `gesture_control.py` do przypisania gestÃ³w do skrÃ³tÃ³w klawiszowych.

System analizuje w czasie rzeczywistym obraz z kamery OAK-D, przetwarza go przez MediaPipe,
zbiera sekwencjÄ™ 3D punktÃ³w dÅ‚oni (21 landmarkÃ³w x [x, y, z]) i klasyfikuje jÄ… za pomocÄ… modelu LSTM.

Wymagania:
- OAK-D + DepthAI
- MediaPipe
- PyTorch
- pyautogui (do obsÅ‚ugi skrÃ³tÃ³w systemowych)

"""

import depthai as dai
import cv2
import mediapipe as mp
import torch
import numpy as np
import time
from collections import deque
from train_gesture_lstm import GestureLSTM
from gesture_control import handle_gesture


# === DEBUG: logowanie gestÃ³w do pliku ===
ENABLE_LOG = True  # â† ustaw na False, aby wyÅ‚Ä…czyÄ‡ logi
LOG_FILE = "gesture_log.txt"

def log_event(message):
    if ENABLE_LOG:
        timestamp = time.strftime("[%Y-%m-%d %H:%M:%S]")
        with open(LOG_FILE, "a") as f:
            f.write(f"{timestamp} {message}\n")


# === Parametry ===
sequence_length = 100
model_path = "gesture_model.pt"
label_map = {0: "shake", 1: "wave", 2: "ok", 3: "thumbs_up"}  # <- zaktualizuj wedÅ‚ug potrzeb

# === Zabezpieczenia przed spamem ===
last_label = None
last_time = 0
cooldown_time = 2  # sekundy
min_stable_count = 5
stable_counter = 0
                    log_event(f"Nowy gest wykryty: {label}")

# === Inicjalizacja modelu ===
model = GestureLSTM()
try:
    model.load_state_dict(torch.load(model_path))
except FileNotFoundError:
    print(f"âŒ Nie znaleziono modelu: {model_path}")
    exit(1)
model.eval()

# === Bufor sekwencji ===
sequence = deque(maxlen=sequence_length)

# === MediaPipe Hands ===
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1,
                       min_detection_confidence=0.5, min_tracking_confidence=0.5)

# === OAK-D Pipeline ===
pipeline = dai.Pipeline()
cam_rgb = pipeline.create(dai.node.ColorCamera)
xout = pipeline.create(dai.node.XLinkOut)
xout.setStreamName("video")

cam_rgb.setBoardSocket(dai.CameraBoardSocket.RGB)
cam_rgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_720_P)
cam_rgb.setFps(30)
cam_rgb.video.link(xout.input)


def run_live_prediction():
    global last_label, last_time, stable_counter

    with dai.Device(pipeline) as device:
        q_rgb = device.getOutputQueue("video", maxSize=4, blocking=False)

        while True:
            in_rgb = q_rgb.get()
            frame = in_rgb.getCvFrame()
            rgb_input = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = hands.process(rgb_input)

            if results.multi_hand_landmarks:
                landmarks = results.multi_hand_landmarks[0]
                coords = []
                for lm in landmarks.landmark:
                    coords.extend([lm.x, lm.y, lm.z])

                sequence.append(coords)

                if len(sequence) == sequence_length:
                    input_tensor = torch.tensor([sequence], dtype=torch.float32)
                    with torch.no_grad():
                        output = model(input_tensor)
                        prediction = torch.argmax(output, dim=1).item()
                        label = label_map.get(prediction, "Unknown")

                    # StabilnoÅ›Ä‡ + cooldown
                    current_time = time.time()
                    if label == last_label:
                        stable_counter += 1
                    else:
                        stable_counter = 0
                    log_event(f"Nowy gest wykryty: {label}")

                    if stable_counter >= min_stable_count and (current_time - last_time) > cooldown_time:
                        handle_gesture(label)
                        log_event(f"Rozpoznano gest: {label} â†’ wykonano akcjÄ™")
                        last_time = current_time

                    last_label = label

                    # WyÅ›wietl nazwÄ™ gestu
                    cv2.putText(frame, f"Gest: {label}", (10, 40),
                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

            cv2.putText(frame, f"Frames: {len(sequence)}/{sequence_length}", (10, 70),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)

            cv2.imshow("ğŸŸ¢ Rozpoznawanie gestÃ³w", frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    cv2.destroyAllWindows()
